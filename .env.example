# Redis Configuration
REDIS_URL=redis://localhost:6379

# Ollama/Llama Server Configuration
LLAMA_SERVER_URL=http://localhost:11434
OLLAMA_HOST=http://localhost:11434  # Used by ollama Python library
DEFAULT_MODEL=deepseek-coder-v2:latest

# OpenTelemetry Configuration
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Optional: Llama.cpp CLI configuration (for local llama.cpp usage)
# LLAMA_CPP_CLI=/data1/llama.cpp/bin/llama-cli
# GGUF_DIR=/data1/GGUF
